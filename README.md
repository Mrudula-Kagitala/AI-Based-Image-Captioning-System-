ğŸ“Œ AI-Based Image Captioning System

This project is a web-based application that automatically generates captions for uploaded images using a Vision-Language Transformer model (BLIP).
It also supports multilingual translation and text-to-speech output, making it accessible for visually impaired and non-English users.

ğŸš€ Features

âœ” Upload an image and generate an automatic caption
âœ” Uses BLIP transformer model for accurate captioning
âœ” Translate captions into multiple languages
âœ” Listen to the caption using text-to-speech
âœ” Simple, responsive web interface

ğŸ§  Tech Stack

Programming Language -- Python
Backend Framework -- Flask
AI Model -- BLIP (Hugging Face Transformers)
Deep Learning Library -- PyTorch
Text-to-Speech -- gTTS
Translation -- deep-translator
UI -- HTML, CSS, JavaScript

ğŸ“ Project Structure

AI-Image-Captioning-System/
â”‚
â”œâ”€ app.py
â”œâ”€ requirements.txt
â”œâ”€ README.md
â”‚
â”œâ”€ templates/
â”‚   â””â”€ index.html
â”‚
â””â”€ static/
    â”œâ”€ style.css
    â””â”€ script.js

â–¶ï¸ How to Run the Project

1ï¸âƒ£ Clone the Repository
https://github.com/Mrudula-Kagitala/AI-Based-Image-Captioning-System-.git

cd AI-Image-Captioning-System

2ï¸âƒ£ Create Virtual Environment
python -m venv venv

3ï¸âƒ£ Activate Environment

Windows:

venv\Scripts\activate


Mac/Linux:

source venv/bin/activate

4ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

5ï¸âƒ£ Run the Application
python app.py

6ï¸âƒ£ Open in Browser
http://127.0.0.1:5000

ğŸ–¼ Sample Output

Input Image	Generated Caption
ğŸ¶ Dog Image	"A dog sitting on the grass"
ğŸš² Bicycle Image	"A bicycle parked on a street"


<img width="1837" height="933" alt="Screenshot 2025-12-05 133434" src="https://github.com/user-attachments/assets/1a9f7f64-7dcd-4ee5-a2f1-e61c2b18d5d8" />


<img width="908" height="885" alt="Screenshot 2025-12-05 133655" src="https://github.com/user-attachments/assets/8c5fcf8d-43ee-4c1a-a61f-f160e724c825" />


<img width="758" height="898" alt="Screenshot 2025-12-05 134217" src="https://github.com/user-attachments/assets/a4d5ac7b-483f-49a3-b548-645d89cd6508" />


ğŸ“š How It Works

The user uploads an image from the browser.

The Flask backend receives the file via REST POST API.

The BLIP model processes the image and generates a caption.

If selected, the caption is translated using deep-translator.

The caption is converted to audio using gTTS and sent back to the frontend.

The user can view the text and listen to the spoken caption.

ğŸ›  Future Enhancements

ğŸ”¹ Deploy to cloud with GPU support
ğŸ”¹ Add offline translation and offline TTS
ğŸ”¹ Improve UI with drag-and-drop image upload
ğŸ”¹ Store user history using a database
